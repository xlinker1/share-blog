---
draft: false
tags:
---

## 矩阵乘

在实现矩阵乘的反向传播时，才忽然发觉自己其实不太清楚应该怎么乘。整理思路，记录一下，可能有点不太严谨。
先是结论

$Y=WX$
$X_\text{grad}=W^T Y_\text{grad}$
$W_\text{grad} = Y_\text{grad} X^T$

这里的dX表示X矩阵中各个元素微小的变化，可以表示梯度grad_X的方向。也就是说`grad_X=W.transpose() @ grad_Y`因为梯度的物理含义就是输入x向哪个方向微小的变化后，输出变化最大
比如： $y=f(x_1,x_2,x_3)$
$$dy=
\begin{bmatrix}
 \frac{\partial y}{\partial x_1}  &  \frac{\partial y}{\partial x_2} &
 \frac{\partial y}{\partial x_3}
\end{bmatrix}
\begin{bmatrix}
 dx_1\\
dx_2 \\
dx_3
\end{bmatrix}
$$
想最高效的通过变动x让y变大，就是说要在给定dx向量的模长时，让dy最大。这时dx向量与梯度同向
$$
\begin{bmatrix}
 dx_1\\
dx_2 \\
dx_3
\end{bmatrix}\propto 
\begin{bmatrix}
 \frac{\partial y}{\partial x_1}  \\  \frac{\partial y}{\partial x_2} \\
 \frac{\partial y}{\partial x_3}
\end{bmatrix}

$$

如果g是一个多输入多输出的函数 $(y_1,y_2,y_3)=g(x_1,x_2,x_3)$
$$
\begin{bmatrix}
 dy_1\\
dy_2 \\
dy_3
\end{bmatrix}=
\begin{bmatrix}
 \frac{\partial y_1}{\partial x_1}  &  \frac{\partial y_1}{\partial x_2} &
 \frac{\partial y_1}{\partial x_3} \\
  \frac{\partial y_2}{\partial x_1}  &  \frac{\partial y_2}{\partial x_2} &
 \frac{\partial y_2}{\partial x_3} \\
  \frac{\partial y_3}{\partial x_1}  &  \frac{\partial y_3}{\partial x_2} &
 \frac{\partial y_3}{\partial x_3} 
\end{bmatrix}
\begin{bmatrix}
 dx_1\\
dx_2 \\
dx_3
\end{bmatrix}
$$
中间那个矩阵就是这个多输入多输出函数的梯度，这个矩阵也叫雅可比矩阵 (Jacobian matrix)
当$y=Wx$时(y,x为对应列向量)，雅可比矩阵就是W

面对输入输出非常多的情况时，这个矩阵会非常大。为了避免这个问题，深度学习一般都只是求多输入单输出函数的梯度。即使面对一个多输出的函数，也会在把多输出转化成一个单一的损失，如下所示。如果对下面的函数g和f都取微分，这就相当于在原来的雅可比矩阵左边乘一个行向量。

比如：$(y_1,y_2,y_3)=g(x_1,x_2,x_3), l=f(y_1,y_2,y_3)$

$$
dl=
\begin{bmatrix}
 \frac{\partial l}{\partial y_1}  &  \frac{\partial l}{\partial y_2} &
 \frac{\partial l}{\partial y_3}
\end{bmatrix}
\begin{bmatrix}
 dy_1\\
dy_2 \\
dy_3
\end{bmatrix}
=
\begin{bmatrix}
 \frac{\partial l}{\partial y_1}  &  \frac{\partial l}{\partial y_2} &
 \frac{\partial l}{\partial y_3}
\end{bmatrix}
\begin{bmatrix}
 \frac{\partial y_1}{\partial x_1}  &  \frac{\partial y_1}{\partial x_2} &
 \frac{\partial y_1}{\partial x_3} \\
  \frac{\partial y_2}{\partial x_1}  &  \frac{\partial y_2}{\partial x_2} &
 \frac{\partial y_2}{\partial x_3} \\
  \frac{\partial y_3}{\partial x_1}  &  \frac{\partial y_3}{\partial x_2} &
 \frac{\partial y_3}{\partial x_3} 
\end{bmatrix}
\begin{bmatrix}
 dx_1\\
dx_2 \\
dx_3
\end{bmatrix}
=
\begin{bmatrix}
 \frac{\partial l}{\partial x_1}  &  \frac{\partial l}{\partial x_2} &
 \frac{\partial l}{\partial x_3}
\end{bmatrix}
\begin{bmatrix}
 dx_1\\
dx_2 \\
dx_3
\end{bmatrix}
$$

令$y_\text{grad}=\begin{bmatrix} \frac{\partial l}{\partial y_1}  \\  \frac{\partial l}{\partial y_2} \\ \frac{\partial l}{\partial y_3}\end{bmatrix}$

上面的式子就表示，通过对$y_\text{grad}$右乘$g$对应的雅可比矩阵(如果g是矩阵乘就是W)，我们求得了$x_\text{grad}$，也就是x相对于输出的梯度

所以$y=Wx$时，$x_\text{grad}^T=y_\text{grad}^T W$，转置后就是$x_\text{grad}=W^T y_\text{grad}$

因为左乘是行变换，所以对$Y=WX$，X的每列之间无关，所以$X_\text{grad}=W^T Y_\text{grad}$

对$Y=WX$和$X_\text{grad}=W^T Y_\text{grad}$两边同时转置，然后变量代换，可以推得$W_\text{grad}=Y_\text{grad}X^T$



