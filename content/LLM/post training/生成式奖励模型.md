---
draft: false
tags:
---


## 奖励与奖励分配

什么是奖励？
在一般的强化学习里


什么是奖励分配？
在







现在调语言模型基本都是策略梯度。这样不管强化学习的训练算法是啥，准确的奖励模型都很重要。
1. 尝试用语言模型生成奖励
2. 尝试用chain of thought加强奖励的质量
3. 尝试用chain of thought做高质量的奖励分配。我获得这么多奖励，究竟是因为什么？我哪一步做对了？之前没有语言模型的时候，只能采用GAE加大量采样来解决这一问题。现在语言模型有了一定的推理能力，应该利用这一点。

## RLAIF





## Rule Based Rewards

[openai.com/index/improving-model-safety-behavior-with-rule-based-rewards/](https://openai.com/index/improving-model-safety-behavior-with-rule-based-rewards/)
细粒度的对





## 从生成式奖励模型获取token level reward

[GitHub - RUCAIBox/RLMEC: The official repository of "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"](https://github.com/RUCAIBox/RLMEC)
这篇文章说是token level reward，但如果认为最终目标是提升正确解答的概率，那么应该是token level advantage function。利用生成式奖励模型做奖励分配的思路非常对。但是本文具体分配奖励的方法实在太糙了，完全是拍脑袋的使用生成token的概率。
总结如下
1. 给gpt-4输入（问题，正确答案，当前语言模型生成的错误答案），让它找出错误答案出现的位置，把错误答案修改正确，要求尽可能不要修改错误答案没错的地方。然后用这些数据对instruct finetune的模型微调，让它模仿gpt-4，也输出对错误答案的修改。作为奖励模型
2. 利用奖励模型输出token的概率作为奖励。
3. 训练：使用奖励模型输出对应位置的token的奖励作为该token处的梯度的权重。同时使用ppo特有的梯度裁剪方法。点进代码看，发现就是带token梯度加权和裁剪的sft，令人失望。




最后，随着模型能力越来越强，奖励模型会越来越倾向于直接in context learning，而不是调模型参数。但究竟使用哪种其实取决于性价比。