---
draft: false
tags:
---

next token prediction等于压缩的原因是对每个词预测的越准，词的不确定度就越小，平均每个词的熵就越小。平均每个词的最短编码长度就越小。



[Prediction and entropy of printed English - ‎Shannon 1951](https://ia904704.us.archive.org/15/items/bstj30-1-50/bstj30-1-50.pdf)


$$
\begin{aligned}
F_N&=-\sum_{i,j}p(b_i,w_j)\log_2 p(w_j|b_i)\\
&=-\sum_{i,j}p(b_i,w_j)\log_2p(b_i,w_j)+\sum_ip(b_i)\log p(b_i)
\end{aligned}
$$

其中$b_i$是任意一个n-1个词的序列，$w_j$是下一个词，$p(w_j|b_i)$是给定上文N-1个词的情况下，对下一个词的条件概率，或者说是一个N-gram的语言模型。
上面的$F_N$其实就是条件熵，$F_N=H(w|b)=H(w,b)-H(b)$。也就是在这种语言中，连续N个词的序列的熵，减去N-1个词的序列的熵。


[[压缩算法入门]]

qemu,ffmpeg作者写的压缩工具： [ts\_zip: Text Compression using Large Language Models](https://bellard.org/ts_zip/)


---
[2306.04050.pdf](https://arxiv.org/pdf/2306.04050.pdf)

[GitHub - erika-n/GPTzip: An implementation of LLMzip using GPT-2](https://github.com/erika-n/GPTzip)
