---
draft: false
tags:
  - todo
  - 强化学习
---



# 笔记

主要记录自己的理解，尽量少用公式。
因为是新入门，所以都比较基础。但基础的东西才最重要，因为它们面对的问题和它们提出的idea也同样基础。很多东西都可以从它们上面改良组合而成。

## 基础组成

### 基本概念

[Part 1: Key Concepts in RL — Spinning Up documentation](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)

一般强化学习都是希望解决一个序列决策问题，想学习一个策略，最大化序列的整体奖励。
一段决策轨迹一般这样表示 (s,a,s,a,s,a)。s表示state，大多数时候真实状态无法被完全观测到，因此也可以写成observation，用o来表示。a表示action，可以是离散动作或者连续动作，在棋牌游戏中，不同state后面能跟的动作空间也会不同。
有时会使用(s,a,r,s,a,r)，其中r表示状态s下做动作a所获得的奖励，或者r(s,a)。而强化学习的目标就是最大化奖励。奖励决定了人们想让强化学习学到啥，没有奖励或者奖励太少就什么也学不到或者很难学，因此有时需要人为仔细设计。

既然是用策略最大化奖励，就需要描述策略和对应获得的奖励。

**策略函数**：$\pi(a|s)$输入当前状态s，输出动作a的概率分布。因为真实状态无法完全被观测到，所以有时会把当前状态前面的所有动作状态序列作为输入，或者像rnn一样维持一个隐藏状态。
输出动作最简单的情况就是离散动作，softmax一下就好。有些时候动作空间过大，或者是连续动作空间的话，可能需要特殊处理。

**V函数**：$V_\pi(s)$表示在使用策略$\pi$的情况下，从当前状态$s$往后接着玩，得到的预期奖励是多大（奖励的期望）。它表示在当前策略的眼中，这个状态s有多好。

**Q函数**：$Q_\pi(s,a)$表示从当前状态$s$往后做一个动作$a$，然后往后用策略$\pi$接着玩，得到的预期奖励有多大。Q函数和V函数可以互相推导。
由V函数的定义，从Q函数可以算出V函数。$V_\pi(s_t)=\mathbb{E}_{\pi(a|s_t)} \left[ Q_\pi(s_t,a) \right]$
而Q函数也可以由下一个状态的V函数得到，$Q_\pi(s_t,a_t)=r(s_t,a_t)+V(s_{t+1})$. 其中$s_{t+1}$是环境给出来的下一个状态，即$s_{t+1}=e(s_t,a_t)$. 
大多数时候下一个状态$s_{t+1}$是不确定的，$Q_\pi(s_t,a_t)=r(s_t,a_t)+\mathbb{E}_{e(s_{t+1}|s_t,a_t)} \left[ V(s_{t+1}) \right]$

另外，我觉得这两个方程其实很对称。它们涉及对两个概率分布求期望，V函数是对s->a的动作概率分布求期望，Q函数是对(s,a)->s的环境转移概率分布求期望。而奖励r既可以认为是环境s根据动作a给的（$r(s_t,a_t)$），也可以认为是actor根据观测到的下一个状态$s_{t+1}$自己给的（$r(s_{t+1})$）

### 基本问题

Explore and exploit（探索和利用）
即使是多臂老虎机也会存在探索和利用的问题
面对一个未知的环境，和固定的尝试次数。应该分配多少次尝试在探索上？多少在利用上？怎样才能最大化奖励？
如果探索过少，急于利用，就会卡在局部最优。反过来，则无法提升利用效率。


Credit assignment（信用分配/奖励分配）
只要是一个序列决策，之后的奖励来自于之前某个时间的动作，就存在奖励分配的问题。
一个系统里，各个部分对整体的性能提升分别起着多大作用？
一个轨迹里，一个动作对整体获得的奖励有多大贡献？你如何确定当前获得的奖励是之前哪一步造成的？
#todo 


奖励稀疏问题


样本效率问题

### 算法

算法的关键问题是，如何提升策略？
Ilya在一次公开课上总结的非常好，“先随机试试，如果结果好，就增大这个动作的概率”，这是策略梯度的做法。
#### 策略梯度
[[策略梯度算法]]


#### DQN
不同于policy gradient直接训练策略函数来表示策略，DQN主要采用Q函数来表示策略。
按照定义，任何一个策略$\pi$，都会决定一个Q函数。
而$Q_\pi(s,a)$表示在s状态下做动作a的“价值”，那么当前步只需要找到令$Q_\pi(s,a)$最大的动作a，就可以在当前状态s下最大化价值了，这样从当前状态s接着玩到结束获得的价值比原策略$\pi$大。于是我们可以根据这个Q函数获得一个新的确定性策略$\pi'(s)=\max_a Q_\pi(s,a)$，它总比原来的策略$\pi$好一点。
于是我们可以用神经网络随机初始化一个Q函数，它对应一个原策略$\pi$，同时对应着一个更好的策略$\pi'$. 然后就可以基于$\pi'$获得一个更好的Q函数，然后迭代的训练出更好的Q函数。

接下来的问题就是如何根据$\pi'$去训练对应的$Q_{\pi'}$了。
$Q_{\pi'}(s_t,a_t)=r(s_t,a_t)+V_{\pi'}(s_{t+1})$. 注意$\pi'$是确定性策略，$a_{t+1}=\pi'(s_{t+1})=\arg\max_a Q_\pi(s_{t+1},a)$
因此$Q_{\pi'}(s_t,a_t)=r(s_t,a_t)+\max_a Q_\pi(s_{t+1},a)$
训练数据为$(s_t, a_t, r_t, s_{t+1})$

这样就只需要让新网络拟合旧网络的输出就可以了。什么时候等式成立，即$Q(s_t,a_t)=r(s_t,a_t)+\max_a Q(s_{t+1},a)$，就得到了最优策略，最优Q函数。此时$\pi(s_{t+1})=\arg\max_a Q_\pi(s_{t+1},a)$

感觉公式还是太多了....可能不太好理解。提供一个最简单的理解方式，所谓强化学习就是强化好的动作，在上面的算法里，argmax就是迭代选取好动作的过程。


优点：
- 最大的好处是可以使用旧的采样轨迹来迭代计算Q. 也就是说off-policy，训练用轨迹与策略无关。
不足：
- 为了提升训练的稳定性，不同算法有各种各样的改良。
- 动作空间连续时，不好对动作算argmax. 
- 迭代Q函数时，因为目标值存在max操作。误差累积会让Q函数估计的价值相对实际价值越来越高。
思考：
- 要让神经网络参数化的Q函数准，应该让训练集和测试集分布相同，应该要让采样轨迹包含尽可能多样的(s,a,r,s)。
- 如果环境会变，需要采样新轨迹。比如self-play等情况（这不废话吗orz）
- 如果奖励稀疏还能训练好吗？
应该可以结合显式的策略函数进行改良。 #todo 
按印象写了一下，为了更好的理解Q函数的迭代过程，可以在之前加入一些简单的例子....别人的教程应该有，之后再加吧....
主要是这类提升策略的思路可以和很多东西组合，设计新的强化学习算法。

#### 蒙特卡洛树搜索

##### UCB
UCB（Upper Confidence Bound）来自于一个基础的“多臂老虎机问题”，关键是探索与利用的取舍。
你面前有一排按钮，每按一次会获得一个奖励，你一共能按的次数是有限的，目标是获得尽可能多的奖励。按钮之间相互独立，获得的奖励只和按了哪个按钮有关，与按按钮的顺序无关。

假如按每个按钮获得的奖励是确定的，那么我只需要全按一遍(探索)，找到最大奖励的按钮，然后一直按它就行了(利用)。（顺便再假设尝试次数远大于按钮数（不然就是另一道题了233））

但问题在于每个按钮获得的奖励是随机的。在这种情况下，我只需要尽快找到奖励均值最大的按钮，然后一直按就行了。
UCB对每个按钮 根据实际获得的奖励 估计实际奖励均值的Upper Confidence Bound。Upper Confidence Bound的意思是有多大概率(如95%)实际奖励均值小于UCB。所以UCB其实是实际奖励均值的上限估计。（想要估计这个UCB需要对奖励服从的概率分布进行一些先验假设，一般是高斯，然后推导出UCB的计算式子。式子基本上是当前奖励均值+奖励方差乘一个系数）
策略是每次选择UCB最大的按钮。每次选择一次都会对当前按钮的UCB进行更新。
（写的太仓促了 #todo )

补充：
UCB（upper confidence bound）是一个很简单的explore-exploitation-tradeoff的解决方案。
[rl中多臂老虎机的理论结果](https://www.zhihu.com/question/312164724/answer/2844833107)


**UCT（Upper Confidence Bound Apply to Tree）**
想象一下上面的按钮游戏被扩展了。按钮之间通过树形组织起来。每按一个按钮，如果该按钮不是叶节点，就会选择一个子按钮按下。如果每一层按钮都按照上面UCB的方式来估计价值，那么最终每个按钮的价值期望会等于它子节点按钮的价值期望的最大值。根节点按钮价值期望会等于所有叶子节点中价值期望最大的那个。

在树搜索中，我们还需要扩展节点 #todo 
（alpha-beta剪枝，min-max.... ）

#### MuZero

[[MuZero相关工作]]

[[Muzero的直观理解]]

核心问题是，它怎么策略提升的？
1. 通过兼顾广度和深度的mcts得到更好的价值估计，还有与该价值对应的节点访问频率。（为什么能获得更好的价值估计？因为上面讲的UCB）
2. 因为期望价值更高了，上面的“对应节点访问频率”就是更好的策略，可以基于它对当前步进行下棋，或者把它作为标签蒸馏到策略网络里。

看了Muzero的伪代码后，发现了一个细节。就是dynamic函数$(s_{t+1},r_t)=g(s_t,a_{t+1})$输入隐藏状态和动作，预测下一个隐藏状态向量。这个隐藏状态向量是确定的。也就是说，它假设下一时刻状态可以由当前状态和动作确定，也就是确定性环境。围棋之类的在树搜索时的价值计算方法和原版alphago一样。
也许后面有改进，但先跳过吧，感觉暂时用不上呀。




#### SAC

[[SAC笔记]]






# 其它
### ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters
[\[2205.01906\] ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters](https://arxiv.org/abs/2205.01906)
[GitHub - nv-tlabs/ASE](https://github.com/nv-tlabs/ASE)
用GAN生成对抗的思路从已有动作数据中学会生成动作。


鉴别函数：
可以理解为引导动作风格靠近已有动作数据

解码器：
为了让隐藏向量和生成动作能一一对应。可以理解为引导动作多样性

利用鉴别函数和解码器作为奖励，用强化学习训练策略$\pi$。
一般gan是用鉴别函数和解码器作为损失，这里是用作奖励来指导策略的学习。思路还是比较直接的，但实际要训练出效果来应该不太容易。

伪代码如下：
- 采样轨迹，计算奖励
	- 从先验分布中采样隐藏向量Z（一般是高斯分布），在这个条件Z下控制策略$\pi$在环境下生成动作序列。奖励由鉴别函数和解码器组合而成，鉴别函数为了奖励动作风格，解码器为了让隐藏向量和生成动作能一一对应。
- 用强化学习ppo训练策略
- 训练鉴别函数和解码器。
	- 鉴别函数：输入为轨迹序列，如果输入是生成动作轨迹则为0，输入是真实动作则为1
	- 解码器：输入为生成轨迹序列，输出为对应的隐藏向量Z，和真实隐藏向量做MSE


训练好之后，就可以把隐藏向量Z当成动作，在其上面训练更高层次的策略了。

### elo分数
[[elo分数]]

### 项目学习

[[TimeChamber项目学习记录]]









## 其它

### 训练小鸟表演才艺

[才艺展示\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1p24y1g7E9)
看小鸟做了这么多动作，不禁想究竟是主人怎么训练它的。
最直观的办法就是在成功完成动作后进行奖励。或者用手辅助，让它先能基本完成动作（模仿学习？），再慢慢通过奖励引导它更好的完成动作，随着动作不断熟练，再调整奖励来提升对动作精度/速度的要求。

这就忍不住让人联想到ppo了，ppo等策略梯度算法通过偏好函数训练$\pi(a|s)$。相当于通过统计的方法，来对每个中间状态的动作价值进行估计（即偏好函数$A(s,a)$）。如果某个状态下做某个动作是关键步骤，那么所有统计出来的轨迹都会给出一个高的偏好函数值。如果某个状态下做的动作只是偶尔获得了高回报，在统计上看也不会有特别高的偏好函数值。如果某个状态已经基本确定会获得高奖励了，那偏好函数因为减去了$V(s)$，因此也不会对其中的动作特别鼓励。这其实是一个奖励分配问题，哪个状态下的哪个动作导致了这个奖励？找出它，然后多做。（具体是通过policy gradient推导+GAE来做的奖励分配）

但是奖励本身并不是目的，我们真正的目的是告诉小鸟要做什么动作。但是小鸟不能理解，听不懂人话，所以只能通过”小鸟尝试动作“”奖励我想要的动作“，来告诉它应该做什么动作。这里就非常像rlhf的思想了，即通过人类偏好训练奖励模型，它替代人对人想要的动作进行奖励，再把奖励模型放到强化学习的框架里去训练。
当然这只有在人一眼能看出来结果好坏的情况下有用，比如人可以轻易知道小鸟的动作完成的好不好，语言模型的回答看上去讨不讨喜。

如果能直接用自然语言告诉小鸟要做什么动作，小鸟就能自己完成任务，根本不需要前面的奖励，或者大大降低需要尝试动作的时间。或者根据推理，想出自己哪个动作不对，哪个动作对了，达成某种奖励分配的效果。
一种可能的设想是给gpt加一些动作token，动作token通过vqvae之类的方法解码成一小段连续动作....？[[#ASE Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters]]


### 人类的奖励分配

忽然冒出来的有趣想法，不知道对不对，但是姑且记录一下

人类习惯使用因果关系来理解时间单向流动的世界。
因果关系能帮助人类来进行快速的in context强化学习，尤其是能够用于奖励分配。
人类的推理能力是从人类对因果关系的理解上面衍生出来的。

想要数据高效，比较精准的做奖励分配，真得靠推理/分析因果关系吧。不然怎么知道哪个动作导致哪个结果呢？不然怎么发现物理定律呢？难道都靠统计吗？

从这点来看，o1真的很有潜力吧。








## 强化学习与大语言模型

[[强化学习与大语言模型]]



