---
draft: false
tags:
---

[在训练神经网络时，反向传播为什么比正向传播慢很多？ - 知乎](https://www.zhihu.com/question/441056428/answer/1699945197)

简单分析一下
$X$是输入数据，行是batch size
$W$是权重

前向计算：
$Y=XW$
假定X的形状为(b, n)，W的形状是(n, p)
FLOPs大概为b\*n\*p

反向计算：
计算输入激活值的梯度 $\dot{X}=\dot{Y}W^{T}$
计算参数W的梯度$\dot{W}=X^{T}\dot{Y}$，并累积到梯度dW上 
做了两次矩阵乘，它们的FLOPs都是b\*n\*p，刚好是2b\*n\*p

梯度累加+优化器更新+模型参数更新：
用dW更新优化器状态，然后更新参数。因为都是对模型参数逐元素计算，正比于模型参数量P，与b(batch size)无关

因此batch size越大，梯度更新的开销越小。
当然这里没有考虑memory bound还是compute bound
逐个元素的参数更新应该是memory bound吧？
所以或许分到多个gpu上比较好。