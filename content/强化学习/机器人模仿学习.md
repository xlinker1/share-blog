---
draft: false
tags:
  - todo
---
起因是刷知乎看到这个回答，里面说“解决这个机器人叠衣服问题，就可以年薪百万！”，我一听就来劲了，这不得试一下？
为什么有些学数学的看不惯甚至鄙视深度学习？ - 多崎作的回答 - 知乎
https://www.zhihu.com/question/58992444/answer/31733750902

首先依据自己弹钢琴的经验，最好是策略梯度算法，目标是最大化奖励，最好与具体动作是什么无关（可能是分层的）。其次是奖励必须要密集，最好是每做一个动作，立刻就能获得奖励，做错了动作，马上就能感觉到不对，因此核心应该在于奖励函数怎么设计。一种思路是判断通过做动作$a_t$获得的下一个观测$o_{t+1}$和自己内心预测的下一个观测$o_{t+1}'$是否一致，越一致奖励就越高，不一致就说明做错了。于是问题就是两部分，一部分是训练一个“内心的愿景”，另一部分是将内心的愿景与实际比较，用策略梯度训练到策略函数里。

一般来说，叠衣服时的观测是视觉，除此之外可能还有手上的传感器之类的。假如只考虑视觉，我们需要以第一人称视角录制大量的机器人正确叠衣服的视频，然后尝试在视频上做某种自回归的训练。在使用时，输入为历史实际观测，输出下一时间片的估计观测，通过对比实际观测和估计观测获得奖励。
考虑到现在文生视频的效果，这应该大概率是可行的。甚至有可能仅仅通过prompt这些训练好的视频生成模型，就可以获得正确叠衣服的视频。

想到这里忽然意识到这么简单的想法别人不可能没做，这很可能是所谓的模仿学习。于是在知乎上查了查，确实有人采用同样的思路做work了。自己毕竟不是专业的，了解不全面，先记录在这里吧
[模仿学习(Imitation Learning)入门指南](https://zhuanlan.zhihu.com/p/140348314)
[\[1707.03374\] Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation](https://arxiv.org/abs/1707.03374)
和我上面的思路几乎一样。没细看 #todo


[Robots that learn | OpenAI](https://openai.com/index/robots-that-learn/)

GAIL等用gan的方法：

![[强化学习笔记#ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters]]



