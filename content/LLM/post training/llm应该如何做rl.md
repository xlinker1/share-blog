---
draft: false
tags:
---
为什么要用强化学习？

人类毫无疑问也有强化学习，而且也是通过类似的方式。有自学过一点钢琴和吉他，想要弹奏出心中的旋律，我会先猜测琴键在哪，一开始尝试可能弹的音偏高了，于是我就往低继续猜，又猜低了，我继续猜测，最后终于按对了。第一天可能弹得磕磕绊绊，第二天就好一点，中间可能因某些原因间隔了几天没弹，可能一周后我忽然想起来去试一下，发现就能很准确的按照心中所想的找到琴键在哪了。
我自己是怎么学会的？首先是猜测琴键在哪，也就是用初始策略函数采样。其次是反馈，我按下琴键听到声音就能感知到对不对。最后是学习，加强按正确琴键的条件概率，降低按错误琴键的条件概率。这种下意识的学习过程很类似于策略梯度。我通过自己的主观感觉（对或者不对）对动作加权，然后人体通过某种内置的方法（可能类似于反向传播）慢慢调整神经网络，睡一觉，或者过几天，人体就学会了如何弹奏内心的旋律。

目前llm中的强化学习也是完全一样的原理。预训练的大模型自身具有很好的泛化能力，同时预训练大模型作为判别器也可以模拟人类对各种方面“对错”的“感觉”，又可以以人类几万倍的速度去做梯度下降。只要细节正确，超越人类是很容易的事情。
但还是面对几个问题：
1. 就是如何持续学习的问题。面对新的环境，如何尽可能的利用旧知识获得某种泛化性，不要死记新知识，如何在学习新知识的时候尽量不要“灾难性遗忘”。（这是那种谁也无法肯定就一定可行的事情。就好像最开始没人知道神经网络训练集损失能下降，训练集损失下降也不一定意味着测试集损失能下降，在充分训练的条件下模型参数增加之后测试集会出现double descent，模型和数据规模扩大后损失会可预测的下降）
2. 奖励分配(credit assigment)的问题。在ppo中通过[[GAE：广义优势函数估计]]和大量采样解决。（DQN和alphago不清楚，不太懂）在大模型这里应该怎么做才能在样本高效的同时效果好？
3. 设计奖励函数，让模型与人类行为和价值观对齐。

所以我相信目前这种策略梯度的rl(人类或ai反馈)，是通往agi的第一步。

从语言模型的角度看，为什么要强化学习？
因为它需要接受一个它完全未知的外界的反馈。


[rl-for-llms.md · GitHub](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)
让模型知道自己不知道什么，鼓励指出不理解的地方并提出问题（向工具或者人类）


[[OpenAI o1的PRM]]
在更短的时间窗口内对模型做监督，在更短的时间窗口内使用强化学习。

[[生成式奖励模型]]
让模型通过事后反思来得知哪一步真正有价值，哪一步对之后的平均奖励提升最多(优势函数)。最高效的credit assigment肯定是归因分析。

[Scaling Laws for Reward Model Overoptimization — LessWrong](https://www.lesswrong.com/posts/shcSdHGPhnLQkpSbX/scaling-laws-for-reward-model-overoptimization)
KL散度约束真的有必要吗？是不是early stopping或者单纯的降低learning rate会更好？



[Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification | OpenReview](https://openreview.net/forum?id=UXuBzWoZGK&noteId=YiStZYdof7)


### post-training的泛化性与持续学习

现在的常见范式都是在pretrain的模型上微调。不管是强化学习还是sft，微调的本质就是想办法利用预训练模型的泛化性。
强化学习本身只是一个评估序列决策智能体的框架，围绕它的各种算法都没有关注泛化性，甚至里面使用的网络都不一定要是神经网络，因为它是通过大量采样训练的，测试时的情况很可能训练时通过采样已经知道了。

持续学习则指的是如何让模型在适应新环境(新数据集)的时候，避免对原始能力造成破坏，即“灾难性遗忘”。

我的猜想是，如果微调时越能充分利用原始的能力(更好的利用泛化性)，就越能保留原始能力(避免灾难性遗忘)



[\[2411.07681v1\] What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?](https://arxiv.org/abs/2411.07681v1)
论文中指出，在用sft教语言模型做推理时，更小的学习率会影响模型的learning dynamic，并加强语言模型在数据上的的泛化能力。论文中使用pre-memorize accuracy来衡量泛化能力，指的是在完全记住训练推理路径(perplexity降到一个阈值以下)之前，模型在训练集问题上自己采样用不同路径得到正确答案的正确率。论文中发现pre-memorize accuracy和测试集正确率正相关。

这意味着我们所期望的，有泛化性的，而不是死记硬背的学习可能发生在perplexity大幅下降之前。面对着带噪声的监督，比如带噪声的奖励，weak to strong中弱模型生成的监督数据，我们希望模型在训练时忽略掉噪声，专注于我们真正想传达给模型的内容，专注于自己能理解并认可的部分。这时就需要利用模型自身的泛化性。比如，用小学习率训练多个epoch会更好吗？于是可以设计一个类似weak to strong的实验，使用带噪声的数据，用不同学习率去微调强模型，观察微调后性能变化。




[Can LLMs learn from a single example? – fast.ai](https://www.fast.ai/posts/2023-09-04-learning-jumps/)
这篇博客提出了sft中的一个现象。就是模型能在短短的一个epoch内很好的记住训练数据，学习率越高记得越好。证据就是在一个epoch后，training loss呈阶梯状大幅下降。而随着epoch增多，val loss则呈现阶梯状上升，说明出现了一定的过拟合。
